# Deep Dive Final Report: Advantage Actor-Critic (A2C)

## Project Overview

This is my final project for Data Structures and Algorithms, F25. Following up on [my midterm project, in which I implemented the Q-learning algorithm](github.com/itannermahncke/stochastic_q-learning_navigation/), I dove deeper into reinforcement learning by exploring actor-critic algorithms. I learned about the foundational math behind actor-critic, and I implemented the Advantage Actor-Critic algorithm myself for a test enviroment. Finally, I reflected on the sucess of my A2C agent and considered further improvements to my implementation.

## Introduction to Actor-Critic

### Actor-Critic Contextualized

Actor-Critic is a family of algorithms that falls under the umbrella of Reinforcement Learning (RL).

In RL, decision-making "agents" learn to achieve some desirable goal in their environment while avoiding costly penalties. The agent has a set of actions it can take; depending on the state that the agent is in, those actions will result in different costs. For example, moving to the left is rewarding for the agent if it is to the right of its goal, but that same action is very costly if the agent is instead to the right of a cliff! The agent repeats this process over and over, with each iteration described as an "episode". After several episodes, the agent will have learned an optimal action sequence for achieving its goal with minimal cost.

The two main styles of Reinforcement Learning are policy-based methods and value-based methods. Policy-based methods explicitly build a representation of a policy, which is a function that maps states to optimal actions. During learning, parameters of the policy function are tuned until an optimal policy is found. Value-based methods do not attempt to model the policy directly; instead, they seek to imitate the optimal policy by modeling the value function that maps state-action pairs to short- and long-term rewards. Value-based methods are great for simpler, discrete scenarios, but struggle in situations with high dimensionality.

The Actor-Critic algorithm family employs a combination of policy-based and value-based methods for even stronger results.

### The Actor-Critic Algorithm

In this family of algorithms, the Actor component learns a policy function while the Critic learns a value function. At some state $S_t$, the Actor uses its current policy approximation to take some action $A_t$. The Critic then calculates the value of taking $A$ while in $S$. This value is passed back to the Actor, who uses it alongside the simple reward $R_{t+1}$ produced by the environment for being in state $S_{t+1}$ to update its policy parameters. In this way, the Critic provides the Actor feedback on the actions it takes, leading to more meaningful improvements to its policy.

The Actor is the policy function $\pi_\theta(A_t|S_t)$ that maps states to optimal actions (or probability distributions representing likely optimal actions). The Actor seeks to find the value of $\theta$ that maximizes the expected overall reward over all T timesteps by way of gradient ascent:

$\nabla_\theta J(\theta)=\mathbb{E}_{\pi \theta}[\sum_{j=0}^T\nabla_\theta\ln\pi_\theta(A_j|S_j)\psi_j|S=S_j]$

In this complex equation, there are two key terms. The first is the score function $\nabla_\theta\ln\pi_\theta(A_j|S_j)$, which represents the direction in the parameter space that increases the probability that the Actor will take $A_t$ in $S_t$. The second term $\psi_t$ is a linear sum of reward signals; its specific makeup is dependent on which Actor-Critic algorithm is being used. Across all algorithms, $\psi_t$ contains a value generated by the Critic. The score function is weighted by the reward signal, meaning that the policy gradient itself learns to ascend in directions that result in high-reward actions.
### Advantage Actor-Critic

Advantage Actor-Critic, or A2C, improves upon the initial idea by using the Advantage function as its Critic instead of the Q-value. The Advantage function is simply the difference between the Q-value (representing value of taking action $A_t$ in state $S_t$) and the mean value of all possible actions in state $S_t$:

$A(S_t,A_t)=Q(S_t,A_t)-V(S_t)$

In essence, the Advantage function describes how much better a particular action is compared to the rest of the options available. However, this requires the Critic to estimate multiple value functions. Instead, the Advantage function can be approximated as the Temporal Difference (TD) error, which describes the difference between the predicted and newly observed value of the state:

$A(S_t)=R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$

When the Actor is updating its $\theta$ parameter, the Advantage function is used to signal rewards:

$\psi_t=A(S_t)=R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$

In practice, the parameter $\theta$ is updated after every timestep like so:

$\Delta\theta=\alpha\nabla_\theta\ln\pi_\theta(A_t|S_t)*(R_{t+1} + \gamma V(S_{t+1}) - V(S_t))$

## Methodology

### Environment Selection

I chose to train my A2C agent using the [OpenAI Gymnasium Library](https://gymnasium.farama.org/), which provides built-in environments with set action, observation, and reward spaces. This is a step up from my midterm, in which I built the environment myself. With Gymnasium, the environment was provided for me and I could focus on my algorithm implementation.

Gymnasium has several environments to choose from. Since actor-critic algorithms are well-suited to environments with highly dimensional, continuous observation spaces, I decided to go with the [Box2D Lunar Lander environment](https://gymnasium.farama.org/environments/box2d/lunar_lander/).

![Lunar Lander GIF]("docs/lunar_lander.gif")

In the Lunar Lander environment,an agent learns to land a vehicle upright in a given landing zone without crashing. The action space is discrete, with four actions available:
- do nothing
- fire main engine
- fire left engine
- fire right engine

The observation space is continuous, and contains eight parameters for the agent to track:
- x and y position
- x and y velocity
- angle and angular velocity
- booleans for if each of two legs is touching the ground

Finally, the environment provides rewards (or penalties) to the agent at each timestep for the following traits (or lack thereof):
- nearness to the landing pad
- slow speeds
- uprightness
- contact with the ground
- inactive engines

Ths reward field encourages the agent to quickly reach the landind pad in a way that is smooth and expends as little energy as possible.

### Designing the Actor and Critic

I initialized both the actor and the critic as Pytorch neural networks. For nearly all of my hyperparameters, I chose values that matched the [stable-baselines3 library's hyperparameters for A2C](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html). I did this to simplify debugging and save hyperparameter exploration for after I got a working impementation.

The actor approximates a policy function that maps states to actions; therefore, its input tensor has a size of eight (for eight observed variables) and its output tensor has a size of four (for four actions). In contrast, the critic approximates a value function that maps states to a value scalar; therefore, its input tensor has a size of eight and its output is a single scalar value.

The first hyperparameter I will discuss is the learning rates of the actor and the critic. The learning rate determines how quickly each neural network adjusts its weights in response to new observations. Higher learning rates can lead to high variance and noisiness, while lower learning rates make for slow convergence. In actor-critic algorithms, the critic must learn faster than the actor so that its value estimates can keep up with the actor's rapidly changing policy. On the flip side, the actor must learn slowly to prevent instability in its policy and to encourage confidence in the policy it eventually converges on.

Based on this reasoning, and the stable-baslines3 examples, I chose the following learning rate values:

$\alpha_{actor} = 0.0007$

$\alpha_{critic} = 0.001$

The next hyperparamter I considered is the discount factor. A high discount factor gives future reward estimates considerable weight when calculating the returns of a given action. A low discount factor makes for a greedier agent focused only on immediate rewards. I chose a high discount factor, to encourage the agent to select actions that led to an eventual landing, rather than minimizing immediate penalties:

$\gamma = 0.99$

### Other Design Choices

#### N-Step Returns

While learning about actor-critic algorithms, I considered the following question: should the actor and critic learn after each step taken, or after each completed episode? It turns out that many other people have also debated this, and the answer is "somewhere in between".

1-step (temporal difference, or TD) methods minimize variance in the returns of a given state-action pair due to their frequency, but also introduce bias as they rely on their own estimates of future rewards to calculate value. On the other hand, episodic (monte carlo, or MC) methods remain unbiased but can introduce high variance as outcomes vary wildly from episode to episode.

A happy compromise is an n-step return method, in which some number of steps between 1 and infinty (or episode termination) occur between each update. By choosing to do updates after batches of n steps, I was able to speed up training while producing low-variance, unbiased updates to my actor and critic. I set the batch size like so:

$n_{steps} = 5$

#### Entropy Regularization

Entropy regularization encourages additional exploration in the agent by factoring entropy loss into the actor's learning. Essentially, the actor learns to maximize rewards and maximize entropy, or random exploration, throughout training. This is valuable because it helps prevent the agent from converging early onto a suboptimal or poor policy. By always encouraging random exploration, the agent will try out other policies throughout training, which gives it the opportunity to find better strategies that build on its existing progress.

I set my initial entropy coefficient to zero and tuned it during training:

$entropy=0.0$

#### Generalized Advantage Estimation

In A2C, returns are contextualized by subtracting a baseline average value provided by the criti. This is called the advantage function, and represents the first A in A2C! The advantage function describes how much better a given action is compared to other available actions. However, the critic's value function approximation is imperfect, and can introduce bias the advantage.

Generalized Advantage Estimation (GAE) reduces critic bias by factoring in TD error, which is the difference between the critic's value estimate of a given state and the calculated value of a given state (approximated as the sum of the immediate reward and the discounted future reward). GAE loops backwards through time over calculated advantages, and each time weighting the advantage by the TD error and future GAE advantage estimates. This results in lower variance and bias in the advantage function, which allows the actor to learn more steadily.

GAE requires one parameter, which controls the weighting of future advantages alongside the discount factor. I used the following value:

$\lambda = 0.95$

## Results and Future Work

## Resources
### Intro to A2C
https://huggingface.co/blog/deep-rl-a2c
https://en.wikipedia.org/wiki/Actor-critic_algorithm
https://www.geeksforgeeks.org/machine-learning/actor-critic-algorithm-in-reinforcement-learning/
https://gymnasium.farama.org/tutorials/training_agents/vector_a2c/#advantage-actor-critic-a2c
### RL Discussion
https://stats.stackexchange.com/questions/407230/what-is-the-difference-between-policy-based-on-policy-value-based-off-policy
### Codebase Examples
https://github.com/Lucasc-99/Actor-Critic/blob/master/scripts/cart-pole-a2c.py
https://medium.com/deeplearningmadeeasy/advantage-actor-critic-a2c-implementation-944e98616b
https://github.com/hermesdt/reinforcement-learning/blob/master/a2c/cartpole_a2c_online.ipynb
### RL on Bipedal Walker
https://github.com/galinilin/tf_A3C_BipedalWalker
https://github.com/DLR-RM/rl-baselines3-zoo
https://github.com/huckiyang/DRL-torch-CoRL/blob/master/Char04%20A2C/A2C.py
### Specific Methods
https://milvus.io/ai-quick-reference/what-is-the-difference-between-monte-carlo-methods-and-td-learning
https://shivang-ahd.medium.com/generalized-advantage-estimation-a-deep-dive-into-bias-variance-and-policy-gradients-a5e0b3454dad
