# Deep Dive Final Report: Advantage Actor-Critic (A2C)

## Introduction to Actor-Critic

### Actor-Critic Contextualized

Actor-Critic is a family of algorithms that falls under the umbrella of Reinforcement Learning (RL).

In RL, decision-making "agents" learn to achieve some desirable goal in their environment while avoiding costly penalties. The agent has a set of actions it can take; depending on the state that the agent is in, those actions will result in different costs. For example, moving to the left is rewarding for the agent if it is to the right of its goal, but that same action is very costly if the agent is instead to the right of a cliff! The agent repeats this process over and over, with each iteration described as an "episode". After several episodes, the agent will have learned an optimal action sequence for achieving its goal with minimal cost.

The two main styles of Reinforcement Learning are policy-based methods and value-based methods. Policy-based methods explicitly build a representation of a policy, which is a function that maps states to optimal actions. During learning, parameters of the policy function are tuned until an optimal policy is found. Value-based methods do not attempt to model the policy directly; instead, they seek to imitate the optimal policy by modeling the value function that maps state-action pairs to short- and long-term rewards. Value-based methods are great for simpler, discrete scenarios, but struggle in situations with high dimensionality.

The Actor-Critic algorithm family employs a combination of policy-based and value-based methods for even stronger results.

### The Actor-Critic Algorithm

In this family of algorithms, the Actor component learns a policy function while the Critic learns a value function. At some state $S_t$, the Actor uses its current policy approximation to take some action $A_t$. The Critic then calculates the value of taking $A$ while in $S$. This value is passed back to the Actor, who uses it alongside the simple reward $R_{t+1}$ produced by the environment for being in state $S_{t+1}$ to update its policy parameters. In this way, the Critic provides the Actor feedback on the actions it takes, leading to more meaningful improvements to its policy.

The Actor is the policy function $\pi_\theta(A_t|S_t)$ that maps states to optimal actions (or probability distributions representing likely optimal actions). The Actor seeks to find the value of $\theta$ that maximizes the expected overall reward over all T timesteps by way of gradient ascent:

$\nabla_\theta J(\theta)=\mathbb{E}_{\pi \theta}[\sum_{j=0}^T\nabla_\theta\ln\pi_\theta(A_j|S_j)\psi_j|S=S_j]$

In this complex equation, there are two key terms. The first is the score function $\nabla_\theta\ln\pi_\theta(A_j|S_j)$, which represents the direction in the parameter space that increases the probability that the Actor will take $A_t$ in $S_t$. The second term $\psi_t$ is a linear sum of reward signals; its specific makeup is dependent on which Actor-Critic algorithm is being used. Across all algorithms, $\psi_t$ contains a value generated by the Critic. The score function is weighted by the reward signal, meaning that the policy gradient itself learns to ascend in directions that result in high-reward actions.
### Advantage Actor-Critic

Advantage Actor-Critic, or A2C, improves upon the initial idea by using the Advantage function as its Critic instead of the Q-value. The Advantage function is simply the difference between the Q-value (representing value of taking action $A_t$ in state $S_t$) and the mean value of all possible actions in state $S_t$:

$A(S_t,A_t)=Q(S_t,A_t)-V(S_t)$

In essence, the Advantage function describes how much better a particular action is compared to the rest of the options available. However, this requires the Critic to estimate multiple value functions. Instead, the Advantage function can be approximated as the Temporal Difference (TD) error, which describes the difference between the predicted and newly observed value of the state:

$A(S_t)=R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$

When the Actor is updating its $\theta$ parameter, the Advantage function is used to signal rewards:

$\psi_t=A(S_t)=R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$

In practice, the parameter $\theta$ is updated after every timestep like so:

$\Delta\theta=\alpha\nabla_\theta\ln\pi_\theta(A_t|S_t)*(R_{t+1} + \gamma V(S_{t+1}) - V(S_t))$

## Resources
### Intro to A2C
https://huggingface.co/blog/deep-rl-a2c
https://en.wikipedia.org/wiki/Actor-critic_algorithm
https://www.geeksforgeeks.org/machine-learning/actor-critic-algorithm-in-reinforcement-learning/
https://gymnasium.farama.org/tutorials/training_agents/vector_a2c/#advantage-actor-critic-a2c
### RL Discussion
https://stats.stackexchange.com/questions/407230/what-is-the-difference-between-policy-based-on-policy-value-based-off-policy
### Codebase Examples
https://github.com/Lucasc-99/Actor-Critic/blob/master/scripts/cart-pole-a2c.py
https://medium.com/deeplearningmadeeasy/advantage-actor-critic-a2c-implementation-944e98616b
https://github.com/hermesdt/reinforcement-learning/blob/master/a2c/cartpole_a2c_online.ipynb
### RL on Bipedal Walker
https://github.com/galinilin/tf_A3C_BipedalWalker
https://github.com/DLR-RM/rl-baselines3-zoo
https://github.com/huckiyang/DRL-torch-CoRL/blob/master/Char04%20A2C/A2C.py
